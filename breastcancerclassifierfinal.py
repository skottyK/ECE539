# -*- coding: utf-8 -*-
"""BreastCancerClassifierFinal.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sgKPTL4KrR57czkWBGK120bQ7Vq0zfZQ
"""

# Names : Eric Markey, Kailee Skotty, Eric Schmidt 
# Class : ECE/CS/ME 539 - Introduction to Artificial Neural Networks 
# Assignment : Class Project

from google.colab import drive
drive.mount('/content/drive')

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import f1_score, confusion_matrix, accuracy_score
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neural_network import MLPClassifier
from sklearn.preprocessing import MinMaxScaler
from sklearn.feature_selection import SelectKBest, chi2, RFE
from sklearn.metrics import roc_curve, auc

data_path = "/content/drive/Shareddrives/ECE 539 Project/BreastCancerClassifier/BreastCancerDataSet.csv"
df = pd.read_csv(data_path)
df.head()

"""# Data Preprocessing

Search for missing or unhelpful values within the dataset
"""

missing_values_count = df.isnull().sum()
missing_values_count

"""Drop the "id" feature and the last column labelled "Unamed: 32"
"""

# Data preprocessing
data = df.drop('id', axis=1)
data = data.drop('Unnamed: 32', axis=1)

# Transform the diagnosis column into a binary classification problem
data['diagnosis'] = data['diagnosis'].map({'M':1, 'B':0})

"""Separate the dataset into Target and features that holds the breast cancer diagnosis in target and the features for each person in features"""

target = data['diagnosis']
features = data.drop(['diagnosis'], axis=1)

"""Feature scaling is a normalization technique that scales the features so that they have zero mean and unit variance."""

# Feature Scaling 
features = (features - features.mean())/features.std()
data

"""# Feature Visualization

## Violin Plot
The violin plot is a type of data visualization that shows the density distribution of the data for each feature, and the split violin plot visualizes the distribution of the two classes, 'Malignant (1)' and 'Benign (0)', in each feature. The plot helps to identify the features that are important in distinguishing between the two classes.

We wil use these plots while doing feature selection to ensure we are choosing the correct features to drop. 
"""

data_violin = pd.concat([target,features.iloc[:,0:15]],axis=1)
data_violin = pd.melt(data_violin,id_vars="diagnosis",
                    var_name="features",
                    value_name='value')
plt.figure(figsize=(15,10))
sns.violinplot(data = data_violin, x="features", y="value", hue="diagnosis",split=True)
plt.xlabel('Features', fontsize=16)
plt.ylabel('Value', fontsize=16)
plt.title("Violin Plot of Features 0-14", fontdict={'fontsize': 16, 'fontweight': 'bold'})
plt.xticks(rotation=90)
plt.show()

data_violin2 = pd.concat([target,features.iloc[:,15:30]],axis=1)
data_violin2 = pd.melt(data_violin2,id_vars="diagnosis",
                    var_name="Features",
                    value_name='Value')
plt.figure(figsize=(15,10))
sns.violinplot(data = data_violin2, x="Features", y="Value", hue="diagnosis",split=True)
plt.xticks(rotation=90)
plt.xlabel('Features', fontsize=16)
plt.ylabel('Value', fontsize=16)
plt.title("Violin Plot of Features 15-30", fontdict={'fontsize': 16, 'fontweight': 'bold'})
plt.show()

"""## Correlation Matrix
One way to select features for model training is by using a correlation matrix. Correlation matrices are useful in identifying features that are highly correlated with the target variable and with each other. 

Features with a high correlation coefficient with the target variable should be retained in the model as they are more informative. 

On the other hand, highly correlated features can cause overfitting and may result in reduced model performance. 

To address this, it may be necessary to remove one of the correlated features. In such cases, the feature that has a stronger correlation with the target variable should be selected to be retained in the model.
"""

# Correlation Matrix 
# concat the target and the features
data_corr = pd.concat([target,features],axis=1)
# compute the correlation matrix
corr = data_corr.corr()
plt.figure(figsize =(18,18))
# plot the correlation as a heatmap
sns.set(font_scale=0.7)
sns.heatmap(corr, annot = True,linewidths=.5, fmt= '.1f')

"""# Recursive Feature Elimination with Cross Validation and Random Forest Classification 

To begin our feature selection process, we will use Recursive Feature Elimination with Cross Validation and Random Forest Classification to determine the optimal number of features to use. We will proceed with this value as we compare other methods of feature selection as well. 
"""

from sklearn.feature_selection import RFECV
# split data train 70 % and test 30 %
x_train, x_test, y_train, y_test = train_test_split(features, target, test_size=0.3, random_state=42)

# The "accuracy" scoring is proportional to the number of correct classifications
clf_rf_3 = RandomForestClassifier(random_state = 43) 
rfecv = RFECV(estimator=clf_rf_3, step=1, cv=5 ,scoring='accuracy')   #5-fold cross-validation
rfecv = rfecv.fit(x_train, y_train)

recursiveFE_best_features = x_train.columns[rfecv.support_]

print('Optimal number of features :', rfecv.n_features_)
print('Best features :', recursiveFE_best_features)

n_scores = len(rfecv.cv_results_["mean_test_score"])
plt.figure()
plt.xlabel("Number of features selected")
plt.ylabel("Mean test accuracy")
min_features_to_select = 1
plt.errorbar(
    range(min_features_to_select, n_scores + min_features_to_select),
    rfecv.cv_results_["mean_test_score"],
    yerr=rfecv.cv_results_["std_test_score"],
)
plt.title("Recursive Feature Elimination \nwith correlated features")
plt.show()

# split data train 70 % and test 30 %
recFE_best = features[recursiveFE_best_features]
x_train, x_test, y_train, y_test = train_test_split(recFE_best, target, test_size=0.3, random_state=42)

#random forest classifier with n_estimators=10 (default)
clf_rf_4 = RandomForestClassifier(random_state=43)      
clr_rf_4 = clf_rf_4.fit(x_train,y_train)

ac_4 = accuracy_score(y_test,clf_rf_4.predict(x_test))
print('Accuracy is: ',ac_4)
cm_4 = confusion_matrix(y_test,clf_rf_4.predict(x_test))

# Plot confusion matrix heatmap
with plt.rc_context(rc={'figure.figsize': (10, 7), 'font.size': 14}):
    ax = sns.heatmap(cm_4, annot=True, fmt="d", cmap="Blues", linewidths=.5, cbar_kws={'label': 'Number of Observations'})
    ax.set(xlabel='Predicted Label', ylabel='True Label', title='Confusion Matrix')  # Set labels and title
    plt.show()

"""**Next, we decided to hand drop items based on our interpretations of the correlation matrix and violin plots.** Based off of the recursive feature elimination above, we were told that it was best to leave 14 features. Hence, we dropped 16 features to leave 14. 

We chose to drop: 
- perimiter_mean
- radius_mean
- compactness_mean
- concave points_mean
- radius_se
- perimeter_se
- radius_worst
- perimiter_worst
- compactness_worst
- concave points_worst
- comopactness_se
- concave points_se
- texture_worst
- area_worst
- concavity_worst 
- smoothness_worse 
"""

drop_list1 = ['perimeter_mean','radius_mean','compactness_mean','concave points_mean','radius_se','perimeter_se','radius_worst','perimeter_worst','compactness_worst','concave points_worst','compactness_se','concave points_se','texture_worst','area_worst', 
            'concavity_worst', 'smoothness_worst']
features_drop = features.drop(drop_list1,axis = 1 )        # do not modify x, we will use it later 
features_drop.head()

"""We then printed a new correlation matrix, searching to ensure that the map did not have features that were too similar."""

#correlation map on selected features
data_drop = pd.concat([target,features_drop],axis=1)
f,ax = plt.subplots(figsize=(14, 14))
sns.set(font_scale=0.7)
sns.heatmap(data_drop.corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)

# split data train 70 % and test 30 %
x_train, x_test, y_train, y_test = train_test_split(features_drop, target, test_size=0.3, random_state=42)

#random forest classifier with n_estimators=10 (default)
clf_rf = RandomForestClassifier(random_state=43)      
clr_rf = clf_rf.fit(x_train,y_train)

ac = accuracy_score(y_test,clf_rf.predict(x_test))
print('Accuracy is: ',ac)
cm = confusion_matrix(y_test,clf_rf.predict(x_test))

# Plot confusion matrix heatmap
with plt.rc_context(rc={'figure.figsize': (10, 7), 'font.size': 14}):
    ax = sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", linewidths=.5, cbar_kws={'label': 'Number of Observations'})
    ax.set(xlabel='Predicted Label', ylabel='True Label', title='Confusion Matrix')  # Set labels and title
    plt.show()

"""# Univariate feature selection and Random Forest Classification

Univariate Feature Selection:
scores each feature independently of the other features, then selects the top-ranked features based on predetermined criteria and threasholds.

- computationally efficent method
- does not take into account the interactions between features (may not always be the best method to use for feature selection)

We will now use the previously calculated value of choosing 14 features to try Univariate Feature Selection and Random Forest Classification and determine what kind of accuracy we can get using these features. 
"""

# store column names
col_names = x_train.columns

#scale data and convert back to dataframe
scaler = MinMaxScaler()
x_train_scaled = scaler.fit_transform(x_train)
x_train = pd.DataFrame(x_train_scaled, columns=col_names)

# select top 10 features using chi2 test
select_feature = SelectKBest(chi2, k=14).fit(x_train, y_train)

# transform data to include only top 5 features
x_train_2 = select_feature.transform(x_train)
x_test_2 = select_feature.transform(x_test)

# train and evaluate random forest classifier on reduced feature set
clf_rf_2 = RandomForestClassifier()
clr_rf_2 = clf_rf_2.fit(x_train_2,y_train)
ac_2 = accuracy_score(y_test,clf_rf_2.predict(x_test_2))
print('Accuracy is: ',ac_2)
cm_2 = confusion_matrix(y_test,clf_rf_2.predict(x_test_2))


# Plot confusion matrix heatmap
with plt.rc_context(rc={'figure.figsize': (10, 7), 'font.size': 14}):
    ax = sns.heatmap(cm_2, annot=True, fmt="d", cmap="Blues", linewidths=.5, cbar_kws={'label': 'Number of Observations'})
    ax.set(xlabel='Predicted Label', ylabel='True Label', title='Confusion Matrix')  # Set labels and title
    plt.show()

"""Recusive Feature Elimination with Cross Validation and Random Forest Classification offered the highest accuracy so we will continue with these selected features. 

We will now compare the different techniques for binary classification problems that follows: 
1. Logistic Regression 
2. Support Vector Machines 
3. Decision Trees 
4. K-Nearest Neighbors 
5. Naive Bayes 
6. Multi-Layer MLP

---


"""

from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neural_network import MLPClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC

# Train-Test Split
x_train_70, x_test_30, y_train_70, y_test_30 = train_test_split(recFE_best, target, test_size=0.3, random_state=42)
x_train_80, x_test_20, y_train_80, y_test_20 = train_test_split(recFE_best, target, test_size=0.2, random_state=42)

# Models to compare
models = {
    "Decision Tree": DecisionTreeClassifier(random_state=42),
    "KNN Classifier": KNeighborsClassifier(n_neighbors=6),
    "Logistic Regression": LogisticRegression(random_state=42),
    "MultiLayer MLP Classifier": MLPClassifier(random_state=42, max_iter=600, hidden_layer_sizes=(100, 100), alpha=0.5),
    "SVM Classifier": SVC(random_state=42, probability=True),
    "Naive Bayes Classifier": GaussianNB()
}

# Prepare lists to store accuracies
model_names = []
accuracies_30 = []
accuracies_20 = []

# Prepare lists to store ROC curve data
fprs = []
tprs = []
aucs = []


# Train and evaluate models
for name, model in models.items():
    print(f"Model: {name}")

    print("70% Train / 30% Test")
    model.fit(x_train_70, y_train_70)
    y_pred_30 = model.predict(x_test_30)
    accuracy_30 = accuracy_score(y_test_30, y_pred_30)
    print(f"Accuracy: {accuracy_30 * 100:.2f}%")

    print("80% Train / 20% Test")
    model.fit(x_train_80, y_train_80)
    y_pred_20 = model.predict(x_test_20)
    accuracy_20 = accuracy_score(y_test_20, y_pred_20)
    print(f"Accuracy: {accuracy_20 * 100:.2f}%")

    # Compute ROC curve and AUC
    y_prob = model.predict_proba(x_test_30)[:, 1]
    fpr, tpr, _ = roc_curve(y_test_30, y_prob)
    roc_auc = auc(fpr, tpr)
    fprs.append(fpr)
    tprs.append(tpr)
    aucs.append(roc_auc)

    model_names.append(name)
    accuracies_30.append(accuracy_30)
    accuracies_20.append(accuracy_20)

    print("-" * 30)

# Plot the accuracies
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))
width = 0.35
x = range(len(models))
ax1.bar(x, accuracies_30, width, label="70% Train / 30% Test")
ax1.bar([i + width for i in x], accuracies_20, width, label="80% Train / 20% Test")

# Customize the plot
ax1.set_title("Accuracy Comparisons")
ax1.set_xticks([i + width / 2 for i in x])
ax1.set_xticklabels(model_names, rotation=45)
ax1.set_ylabel("Accuracy")
ax1.legend(loc='lower left')

# Plot ROC curves
for i, (name, fpr, tpr, auc) in enumerate(zip(model_names, fprs, tprs, aucs)):
    ax2.plot(fpr, tpr, label=f'{name} (AUC = {auc:.2f})', lw=2)

# Customize the ROC plot
ax2.plot([0, 1], [0, 1], 'k--', lw=2)
ax2.set_xlim([0.0, 1.0])
ax2.set_ylim([0.0, 1.05])
ax2.set_xlabel('False Positive Rate')
ax2.set_ylabel('True Positive Rate')
ax2.set_title('Receiver Operating Characteristic Curves')
ax2.legend(loc="lower right")

plt.show()